{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eDiHapZQv6hCN2ZyNRogDVjcoxXHZzwa",
      "authorship_tag": "ABX9TyOQftxUMuhm3uAnKSSGHBuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/havihaviplants/Artbox/blob/main/%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9uEc74U6utq",
        "outputId": "4c008096-3db3-415b-b19b-2528d3dfc200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 자신의 Google Drive 마운트하는 코드를 추가하자@\n",
        "mnist = np.load('/content/drive/MyDrive/MY Drive/mnist.npz')\n",
        "x_train = (mnist['x_train'] - np.mean(mnist['x_train'])) / np.std(mnist['x_train'])\n",
        "y_train = mnist['y_train']\n",
        "x_test = (mnist['x_test'] - np.mean(mnist['x_train'])) / np.std(mnist['x_train'])\n",
        "y_test = mnist['y_test']\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_onehot(labels, num_classes):\n",
        " return np.eye(num_classes)[labels]\n",
        "# one-hot vector의 형태로 변환함\n",
        "y_train_onehot = to_onehot(y_train, 10)\n",
        "y_test_onehot = to_onehot(y_test, 10)\n",
        "print(y_train_onehot.shape, y_test_onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kog67G7Y7tJV",
        "outputId": "bc308178-f3c1-466a-e159-c340bd214f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 10) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (28, 28) 크기의 이미지를 (7, 7) 크기의 이미지로 down-sizing (계산 효율을 위해서)\n",
        "# (7, 7) 크기를 (49,) 크기로 reshape\n",
        "x_train_small = x_train[:, ::4, ::4].reshape(-1, 7*7)\n",
        "x_test_small = x_test[:, ::4, ::4].reshape(-1, 7*7)\n",
        "print(x_train_small.shape, x_test_small.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2vg0oWU7vyb",
        "outputId": "5bae0b41-608d-4026-ebe0-3ac29da745c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 49) (10000, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        " def __init__(self, in_features, out_features):\n",
        "  self.weight = np.random.rand(in_features, out_features) # weight random 초기화\n",
        "  self.bias = np.random.rand(out_features) # bias random 초기화\n",
        "\n",
        "  self.grad_w = np.zeros_like(self.weight) # gradients of weight\n",
        "  self.grad_b = np.zeros_like(self.bias) # gradients of bias\n",
        " def forward(self, x):\n",
        "  self.input_x = x\n",
        "  x = np.dot(x, self.weight) + self.bias # wx + b\n",
        "  return x\n",
        "\n",
        " def backward(self, grad_output):\n",
        "  self.grad_w = np.dot(self.input_x.T, grad_output) # weight의 gradient 계산\n",
        "  self.grad_b = np.sum(grad_output, axis=0) # bias의 gradient 계산\n",
        "\n",
        "  grad_input = np.dot(grad_output, self.weight.T) # backprogation을 위한 grad input 계산\n",
        "  return grad_input\n",
        " def zero_grad(self):\n",
        "  self.grad_w.fill(0.)\n",
        "  self.grad_b.fill(0.)\n"
      ],
      "metadata": {
        "id": "-vHN6i-h72lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        " def forward(self, x):\n",
        "  self.input = x\n",
        "  return np.maximum(x, 0)\n",
        " def backward(self, grad_output):\n",
        "  grad_input = grad_output.copy()\n",
        "  grad_input[self.input < 0] = 0 # 0 보다 크면 gradient는 1, 그렇지 않으면 0\n",
        "  return grad_input"
      ],
      "metadata": {
        "id": "cfHg7BeQ8XN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        " def forward(self, x):\n",
        "  exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "  self.probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "  return self.probs\n",
        " def backward(self, grad_output):\n",
        "  batch_size = grad_output.shape[0]\n",
        "  grad_input = np.zeros_like(grad_output)\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    jacobian_matrix = np.diag(self.probs[i])\n",
        "    jacobian_matrix -= np.outer(self.probs[i], self.probs[i])\n",
        "    grad_input[i] = np.dot(grad_output[i], jacobian_matrix)\n",
        "\n",
        "    return grad_input"
      ],
      "metadata": {
        "id": "G5bnWgnE8l_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    self.layer1 = Linear(input_size, hidden_size)\n",
        "    self.relu = ReLU()\n",
        "    self.layer2 = Linear(hidden_size, output_size)\n",
        "    self.softmax = Softmax()\n",
        "  def forward(self, x):\n",
        "    x = self.layer1.forward(x)\n",
        "    x = self.relu.forward(x)\n",
        "    x = self.layer2.forward(x)\n",
        "    x = self.softmax.forward(x)\n",
        "    return x\n",
        "  def backward(self, y_true, y_pred):\n",
        "    grad_output = (y_pred - y_true) / len(y_true)\n",
        "\n",
        "    grad_output = self.softmax.backward(grad_output)\n",
        "    grad_output = self.layer2.backward(grad_output)\n",
        "    grad_output = self.relu.backward(grad_output)\n",
        "    _ = self.layer1.backward(grad_output)\n",
        "  def zero_grad(self):\n",
        "    self.layer1.zero_grad()\n",
        "    self.layer2.zero_grad()\n"
      ],
      "metadata": {
        "id": "rNwRZNxL8-rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        " def __init__(self, params, lr):\n",
        "  self.params = params\n",
        "  self.lr = lr\n",
        " def step(self):\n",
        "  for param in self.params:\n",
        "    param.weight = param.weight - self.lr * param.grad_w\n",
        "    param.bias = param.bias - self.lr * param.grad_b\n",
        "# Loss Function\n",
        "def CrossEntropyLoss(y_true, y_pred):\n",
        "    y_true = np.argmax(y_true, axis=-1) # one-hot vector -> label\n",
        "    y_pred = y_pred[np.arange(y_true.shape[0]), y_true] # 레이블에 해당하는 y_pred만을 가져옴\n",
        "\n",
        "    return -np.mean(np.log(y_pred + 1e-8))"
      ],
      "metadata": {
        "id": "nZ9Ga_YH-Fq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(input_size=49, hidden_size=128, output_size=10)\n",
        "opti = SGD([model.layer1, model.layer2], lr=0.05)\n",
        "epoch = 5000\n",
        "losses = []\n",
        "for i in range(epoch):\n",
        "  y_pred = model.forward(x_train_small) # hypothesis (wx + b)\n",
        "  loss = CrossEntropyLoss(y_train_onehot, y_pred)\n",
        "  losses.append(loss)\n",
        "\n",
        "  model.zero_grad() # gradients를 0으로 초기화\n",
        "  model.backward(y_train_onehot, y_pred) # backprogation으로 gradients 계산\n",
        "  opti.step() # SGD 기반 weight & bias 업데이트\n",
        "\n",
        "  if i % 50 == 0:\n",
        "    print(f\"[{i:4d}] {loss:.8f}\")\n",
        "\n",
        "losses = np.array(losses)\n",
        "#파이토치를 쓰면 텐서플로우로 갈 수 있지만 그 역은 어렵다. 무조건 파이토치를 해야 함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWMr2FgQ-ROq",
        "outputId": "98de454a-2c13-40c2-9195-d0088633c8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0] 8.78975401\n",
            "[  50] 8.78922741\n",
            "[ 100] 8.78871239\n",
            "[ 150] 8.78820882\n",
            "[ 200] 8.78771656\n",
            "[ 250] 8.78723546\n",
            "[ 300] 8.78676535\n",
            "[ 350] 8.78630604\n",
            "[ 400] 8.78585735\n",
            "[ 450] 8.78541908\n",
            "[ 500] 8.78499103\n",
            "[ 550] 8.78457301\n",
            "[ 600] 8.78416479\n",
            "[ 650] 8.78376617\n",
            "[ 700] 8.78337694\n",
            "[ 750] 8.78299687\n",
            "[ 800] 8.78262575\n",
            "[ 850] 8.78226336\n",
            "[ 900] 8.78190950\n",
            "[ 950] 8.78156393\n",
            "[1000] 8.78122646\n",
            "[1050] 8.78089687\n",
            "[1100] 8.78057495\n",
            "[1150] 8.78026050\n",
            "[1200] 8.77995331\n",
            "[1250] 8.77965319\n",
            "[1300] 8.77935996\n",
            "[1350] 8.77907341\n",
            "[1400] 8.77879336\n",
            "[1450] 8.77851964\n",
            "[1500] 8.77825206\n",
            "[1550] 8.77799046\n",
            "[1600] 8.77773466\n",
            "[1650] 8.77748450\n",
            "[1700] 8.77723982\n",
            "[1750] 8.77700047\n",
            "[1800] 8.77676630\n",
            "[1850] 8.77653716\n",
            "[1900] 8.77631290\n",
            "[1950] 8.77609340\n",
            "[2000] 8.77587852\n",
            "[2050] 8.77566813\n",
            "[2100] 8.77546210\n",
            "[2150] 8.77526032\n",
            "[2200] 8.77506265\n",
            "[2250] 8.77486900\n",
            "[2300] 8.77467924\n",
            "[2350] 8.77449327\n",
            "[2400] 8.77431099\n",
            "[2450] 8.77413228\n",
            "[2500] 8.77395706\n",
            "[2550] 8.77378523\n",
            "[2600] 8.77361670\n",
            "[2650] 8.77345139\n",
            "[2700] 8.77328919\n",
            "[2750] 8.77313004\n",
            "[2800] 8.77297385\n",
            "[2850] 8.77282053\n",
            "[2900] 8.77267003\n",
            "[2950] 8.77252226\n",
            "[3000] 8.77237715\n",
            "[3050] 8.77223464\n",
            "[3100] 8.77209466\n",
            "[3150] 8.77195713\n",
            "[3200] 8.77182201\n",
            "[3250] 8.77168924\n",
            "[3300] 8.77155874\n",
            "[3350] 8.77143046\n",
            "[3400] 8.77130436\n",
            "[3450] 8.77118038\n",
            "[3500] 8.77105845\n",
            "[3550] 8.77093855\n",
            "[3600] 8.77082061\n",
            "[3650] 8.77070458\n",
            "[3700] 8.77059043\n",
            "[3750] 8.77047811\n",
            "[3800] 8.77036757\n",
            "[3850] 8.77025878\n",
            "[3900] 8.77015169\n",
            "[3950] 8.77004627\n",
            "[4000] 8.76994247\n",
            "[4050] 8.76984026\n",
            "[4100] 8.76973960\n",
            "[4150] 8.76964047\n",
            "[4200] 8.76954281\n",
            "[4250] 8.76944661\n",
            "[4300] 8.76935182\n",
            "[4350] 8.76925842\n",
            "[4400] 8.76916638\n",
            "[4450] 8.76907567\n",
            "[4500] 8.76898626\n",
            "[4550] 8.76889812\n",
            "[4600] 8.76881123\n",
            "[4650] 8.76872555\n",
            "[4700] 8.76864107\n",
            "[4750] 8.76855776\n",
            "[4800] 8.76847558\n",
            "[4850] 8.76839453\n",
            "[4900] 8.76831458\n",
            "[4950] 8.76823570\n"
          ]
        }
      ]
    }
  ]
}